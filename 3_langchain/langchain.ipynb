{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "349ad720",
   "metadata": {},
   "source": [
    "### Langchain building block\n",
    "- LLMs\tConnects to a language model (e.g. OpenAI, Anthropic, Mistral, etc.)\n",
    "- Prompts\tTemplates to structure input text sent to the LLM\n",
    "- Output Parsers\tConverts LLM output into structured Python objects (e.g., list, dict)\n",
    "- Tools\tExternal functions the LLM can call (retrievers, calculators, APIs)\n",
    "- Memory\tKeeps context across turns (important for chat/agents)\n",
    "- Chains\tSequences of steps (prompt → LLM → output parser)\n",
    "- Agents\tLLM that uses tools and decides what to do next\n",
    "- Retrievers\tVector DB interfaces for RAG (retrieval-augmented generation)\n",
    "- Runnables\tUnified interface for all components (LLMs, functions, chains, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0cdfc4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What is LangChain?', 'text': 'LangChain is a blockchain-based platform that aims to revolutionize the language service industry by connecting companies and individuals with professional translators and interpreters in a secure and transparent manner. It uses smart contracts and a decentralized network to streamline the language translation process and ensure trust and efficiency in language-related transactions.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Q: {question}\\nA:\")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "response = chain.invoke({\"question\": \"What is LangChain?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7225ef5d",
   "metadata": {},
   "source": [
    "### Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b3b93e",
   "metadata": {},
   "source": [
    "### Simple input\n",
    "- without explicit chat role (system/role/assistant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=' blue and clear, with fluffy white clouds scattered across it. The sun is shining brightly, casting a warm glow over everything below. It is a beautiful day, perfect for spending time outdoors and enjoying the beauty of nature.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 44, 'prompt_tokens': 10, 'total_tokens': 54, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Bl8CDcEfEWOBlzb7lR4PW1Uh2pfpD', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--9e241c2c-0530-4053-a1ca-de30fa6dab53-0', usage_metadata={'input_tokens': 10, 'output_tokens': 44, 'total_tokens': 54, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "llm.invoke(\"The sky is\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ca2639",
   "metadata": {},
   "source": [
    "### Chat role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ab2d81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "prompt = [HumanMessage(\"What is the capital of France?\")]\n",
    "\n",
    "response = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41c6f9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d669c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is Paris.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 14, 'total_tokens': 21, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BlE3LkRLY1L9mSZ57el011S0MFamn', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--d7d7f5b5-c839-4d89-861a-fafc8429198f-0', usage_metadata={'input_tokens': 14, 'output_tokens': 7, 'total_tokens': 21, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a7faaf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "system_msg = SystemMessage(\n",
    "    \"You are a helpful assistant that responds to questions with three exclamation marks.\"\n",
    ")\n",
    "\n",
    "human_msg = HumanMessage(\"What is the capital of France?, just answer the name of the capital only. no other characters like !\")\n",
    "response = model.invoke([system_msg, human_msg])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7469d0d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dda55821",
   "metadata": {},
   "source": [
    "### Prompt Template - Prompt with dynamic input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3359ad61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "template = PromptTemplate.from_template(\"\"\"Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don't know\".\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer: \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2fe2dc",
   "metadata": {},
   "source": [
    "### Fills in a prompt template with actual values\n",
    "- You can call .format() or .invoke() on the template with values before passing it to the model.\n",
    "- .format()\t- a plain string prompt returns str\n",
    "- .invoke()\t- structured prompt + LLM returns PromptValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0cbe9283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Paris' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 56, 'total_tokens': 57, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BlEZA1BaxraYFDYOa8NCauAsuE1rc', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--b4d8ce40-36b9-4ccb-9d15-e44411dc8760-0' usage_metadata={'input_tokens': 56, 'output_tokens': 1, 'total_tokens': 57, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "prompt = template.format(\n",
    "    context=\"Paris is the capital of France.\",\n",
    "    question=\"What is the capital of France?\"\n",
    ")\n",
    "response = llm.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b4d482b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don\\'t know\".\\nContext: Paris is the capital of France.\\nQuestion: What is the capital of France?\\nAnswer: '\n"
     ]
    }
   ],
   "source": [
    "response = template.invoke({\n",
    "    \"context\":\"Paris is the capital of France.\",\n",
    "    \"question\":\"What is the capital of France?\"\n",
    "})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad301e0",
   "metadata": {},
   "source": [
    "#### use chain for template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e41841d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andi\\AppData\\Local\\Temp\\ipykernel_24544\\3091242567.py:2: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=template)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context': 'Paris is the capital of France.', 'question': 'What is the capital of France?', 'text': 'Paris'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=template)\n",
    "result = chain.invoke({\"context\": \"Paris is the capital of France.\", \"question\": \"What is the capital of France?\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "98dc703d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Paris'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a3fbc1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='AI stands for artificial intelligence, which is the simulation of human intelligence processes by machines, especially computer systems. This includes learning, reasoning, problem solving, perception, and language understanding. AI technology is used to develop systems and machines that can perform tasks that typically require human intelligence, such as visual perception, speech recognition, decision-making, and language translation.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 70, 'prompt_tokens': 11, 'total_tokens': 81, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BlFGuiyWjToZT7zdKQ29NFMUSc8Oo', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--198a72e7-5f3c-4f16-a10b-a2981cc72995-0' usage_metadata={'input_tokens': 11, 'output_tokens': 70, 'total_tokens': 81, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Template with variable\n",
    "from langchain.prompts import PromptTemplate\n",
    "template = PromptTemplate.from_template(\"What is {thing}?\")\n",
    "\n",
    "# Step 2: Fill the prompt with .invoke() (PromptTemplate)\n",
    "prompt_value = template.invoke({\"thing\": \"AI\"})\n",
    "\n",
    "# Step 3: Send to LLM with .invoke() (LLM)\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI()\n",
    "response = llm.invoke(prompt_value)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f2112420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hugging Face, OpenAI, and Cohere offer LLMs.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 132, 'total_tokens': 147, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BlEokRiYDIEFV3zv2jTen2RG461CA', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--826cb5bd-127a-4c77-8646-e2178db4b9ad-0' usage_metadata={'input_tokens': 132, 'output_tokens': 15, 'total_tokens': 147, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "# both `template` and `model` can be reused many times\n",
    "\n",
    "template = PromptTemplate.from_template(\"\"\"Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don't know\".\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer: \"\"\")\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "prompt = template.invoke(\n",
    "    {\n",
    "        \"context\": \"The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\",\n",
    "        \"question\": \"Which model providers offer LLMs?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "response = model.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d835c1",
   "metadata": {},
   "source": [
    "### ChatPromptTemplate - Template for building AI chatbot\n",
    "provide dynamic inputs based on the role of the chat message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "14661b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don\\'t know\".', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"Context: The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Question: Which model providers offer LLMs?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            'Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don\\'t know\".',\n",
    "        ),\n",
    "        (\"human\", \"Context: {context}\"),\n",
    "        (\"human\", \"Question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "response = template.invoke(\n",
    "    {\n",
    "        \"context\": \"The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\",\n",
    "        \"question\": \"Which model providers offer LLMs?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeaba70",
   "metadata": {},
   "source": [
    "### ChatPromptTemplate with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a16c6f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don\\'t know\".', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"Context: The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Question: Which model providers offer LLMs?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# both `template` and `model` can be reused many times\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            'Answer the question based on the context below. If the question cannot be answered using the information provided, answer with \"I don\\'t know\".',\n",
    "        ),\n",
    "        (\"human\", \"Context: {context}\"),\n",
    "        (\"human\", \"Question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt = template.invoke(\n",
    "    {\n",
    "        \"context\": \"The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries, respectively.\",\n",
    "        \"question\": \"Which model providers offer LLMs?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2a23e210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='OpenAI and Cohere offer LLMs.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 137, 'total_tokens': 147, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BlF3geskuFrytG0FUly7DKtnPMTZo', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--956776a4-14ef-4c8e-b0b2-7e0014d46f9d-0' usage_metadata={'input_tokens': 137, 'output_tokens': 10, 'total_tokens': 147, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "model = ChatOpenAI()\n",
    "print(model.invoke(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c26c35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf776f42",
   "metadata": {},
   "source": [
    "### Structured Output\n",
    "- need the LLM to generate a structured output such as JSON, XML, CSV\n",
    "- hand that output off to some other part of the code\n",
    "- use pyndantic for data validation and type enforcement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07233206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first=RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001AFAC559AE0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001AFAC55A170>, root_client=<openai.OpenAI object at 0x000001AFAC558160>, root_async_client=<openai.AsyncOpenAI object at 0x000001AFAC5597E0>, temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), kwargs={'tools': [{'type': 'function', 'function': {'name': 'AnswerWithJustification', 'description': '', 'parameters': {'properties': {'answer': {'type': 'string'}, 'justification': {'type': 'string'}}, 'required': ['answer', 'justification'], 'type': 'object'}}}], 'ls_structured_output_format': {'kwargs': {'method': 'function_calling', 'strict': None}, 'schema': <class '__main__.AnswerWithJustification'>}, 'parallel_tool_calls': False, 'tool_choice': {'type': 'function', 'function': {'name': 'AnswerWithJustification'}}}, config={}, config_factories=[]) middle=[] last=PydanticToolsParser(first_tool_only=True, tools=[<class '__main__.AnswerWithJustification'>])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andi\\anaconda3\\envs\\tsel\\lib\\site-packages\\langchain_openai\\chat_models\\base.py:1669: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class AnswerWithJustification(BaseModel):\n",
    "    answer: str\n",
    "    justification: str\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a16357c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'They both weigh the same, one pound.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What weighs more, a pound of bricks or a pound of feathers\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ddf38ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'They both weigh the same, a pound. The weight of an object is determined by its mass, not the material it is made of. In this case, both a pound of bricks and a pound of feathers weigh the same amount because they both have a mass of one pound.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What weighs more, a pound of bricks or a pound of feathers. give answer and justification\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3e13a5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first=RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001AFAC559AE0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001AFAC55A170>, root_client=<openai.OpenAI object at 0x000001AFAC558160>, root_async_client=<openai.AsyncOpenAI object at 0x000001AFAC5597E0>, temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), kwargs={'tools': [{'type': 'function', 'function': {'name': 'AnswerWithJustification', 'description': '', 'parameters': {'properties': {'answer': {'type': 'string'}, 'justification': {'type': 'string'}}, 'required': ['answer', 'justification'], 'type': 'object'}}}], 'ls_structured_output_format': {'kwargs': {'method': 'function_calling', 'strict': None}, 'schema': <class '__main__.AnswerWithJustification'>}, 'parallel_tool_calls': False, 'tool_choice': {'type': 'function', 'function': {'name': 'AnswerWithJustification'}}}, config={}, config_factories=[]) middle=[] last=PydanticToolsParser(first_tool_only=True, tools=[<class '__main__.AnswerWithJustification'>])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andi\\anaconda3\\envs\\tsel\\lib\\site-packages\\langchain_openai\\chat_models\\base.py:1669: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "structured_llm = llm.with_structured_output(AnswerWithJustification)\n",
    "print(structured_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eedb14ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer='They weigh the same' justification='Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not in their weight.'\n"
     ]
    }
   ],
   "source": [
    "response = structured_llm.invoke(\"What weighs more, a pound of bricks or a pound of feathers\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "864830b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They weigh the same\n"
     ]
    }
   ],
   "source": [
    "print(response.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "80adc678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not in their weight.\n"
     ]
    }
   ],
   "source": [
    "print(response.justification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ea477804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnswerWithJustification(answer='They weigh the same', justification='Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not in their weight.')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4b335b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andi\\AppData\\Local\\Temp\\ipykernel_24544\\690762135.py:1: PydanticDeprecatedSince20: The `json` method is deprecated; use `model_dump_json` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  response.json()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"answer\":\"They weigh the same\",\"justification\":\"Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not in their weight.\"}'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c104171f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andi\\AppData\\Local\\Temp\\ipykernel_24544\\3202056457.py:1: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  response.dict()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer': 'They weigh the same',\n",
       " 'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The difference lies in the volume and density of the materials, not in their weight.'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8219db03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e523864",
   "metadata": {},
   "source": [
    "### Output Parser - CommaSeparatedListOutputParser\n",
    "- Splits a string by commas\n",
    "- Strips whitespace from each item\n",
    "- Returns a list of strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e1c1f9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'banana', 'cherry']\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "response = parser.invoke(\"apple, banana, cherry\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c0e46bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['Apple', 'banana', 'pineapple']}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"List three fruits separated by commas.\")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt, output_parser=parser)\n",
    "print(chain.invoke({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8403fc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7014b81e",
   "metadata": {},
   "source": [
    "### Runnable Interface\n",
    "- invoke : transforms a single input into an output\n",
    "- batch : efficiently transforms multiple inputs into multiple outputs\n",
    "- stream : streams output from a single input as it’s produced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5bdd41f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Hello! How can I help you today?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 10, 'total_tokens': 19, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BlGd3DryDtx3ck2nMrwpwCL3PP6pQ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--e3d4bd78-0414-4444-8eea-3c8f873c2059-0' usage_metadata={'input_tokens': 10, 'output_tokens': 9, 'total_tokens': 19, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI()\n",
    "\n",
    "#invoke : transforms a single input into an output\n",
    "completion = model.invoke('Hi there!')\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3f0434",
   "metadata": {},
   "source": [
    "### Batching\n",
    "send all prompts in a single API request (when supported), and get back multiple responses at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5bdf7f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1: content='Hello! How can I assist you today?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 10, 'total_tokens': 19, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BlGgbvDGu1m1Ec2cjCaqstEgzcooM', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--0da0c2d2-025a-40c9-abac-6e65b4001723-0' usage_metadata={'input_tokens': 10, 'output_tokens': 9, 'total_tokens': 19, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "Response 2: content='Goodbye! Have a great day!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 10, 'total_tokens': 18, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BlGgbsL5PndZo4J5RBCZk0rupf8Gt', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--0dd10c63-0d36-42e6-b911-7a51a4e0307c-0' usage_metadata={'input_tokens': 10, 'output_tokens': 8, 'total_tokens': 18, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "# batch : efficiently transforms multiple inputs into multiple outputs\n",
    "#send multiple prompts at once to the model.\n",
    "completions = model.batch(['Hi there!', 'Bye!'])\n",
    "for i, output in enumerate(completions):\n",
    "    print(f\"Response {i+1}:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7a6a0180",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andi\\anaconda3\\envs\\tsel\\lib\\site-packages\\langchain_openai\\chat_models\\base.py:1669: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi!', 'Goodbye']\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Greeting(BaseModel):\n",
    "    message: str\n",
    "\n",
    "structured_model = model.with_structured_output(Greeting)\n",
    "responses = structured_model.batch([\"Say hi\", \"Say goodbye\"])\n",
    "print([r.message for r in responses])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060673a0",
   "metadata": {},
   "source": [
    "## Without batching\n",
    "- call invoke() multiple times  - making N separate API calls — one per prompt.\n",
    "- This is slower and more expensive due to Network overhead per request, Billing minimums per call (especially with OpenAI), Latency per request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ab8df714",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for prompt in prompts:\n",
    "#    result = model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4651c8",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7763aec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={} response_metadata={} id='run--5db1d357-dc06-43a5-a265-a998ee437265'\n",
      "content='Good' additional_kwargs={} response_metadata={} id='run--5db1d357-dc06-43a5-a265-a998ee437265'\n",
      "content='bye' additional_kwargs={} response_metadata={} id='run--5db1d357-dc06-43a5-a265-a998ee437265'\n",
      "content='!' additional_kwargs={} response_metadata={} id='run--5db1d357-dc06-43a5-a265-a998ee437265'\n",
      "content=' Have' additional_kwargs={} response_metadata={} id='run--5db1d357-dc06-43a5-a265-a998ee437265'\n",
      "content=' a' additional_kwargs={} response_metadata={} id='run--5db1d357-dc06-43a5-a265-a998ee437265'\n",
      "content=' great' additional_kwargs={} response_metadata={} id='run--5db1d357-dc06-43a5-a265-a998ee437265'\n",
      "content=' day' additional_kwargs={} response_metadata={} id='run--5db1d357-dc06-43a5-a265-a998ee437265'\n",
      "content='!' additional_kwargs={} response_metadata={} id='run--5db1d357-dc06-43a5-a265-a998ee437265'\n",
      "content='' additional_kwargs={} response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-3.5-turbo-0125', 'service_tier': 'default'} id='run--5db1d357-dc06-43a5-a265-a998ee437265'\n"
     ]
    }
   ],
   "source": [
    "# stream : streams output from a single input as it’s produced\n",
    "for token in model.stream('Bye'):\n",
    "    print(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0ebcbb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the vast expanse of darkest night,\n",
      "Where stars twinkle with celestial light,\n",
      "Lies a realm beyond our reach,\n",
      "Where mysteries of the universe teach.\n",
      "\n",
      "Galaxies spin and planets dance,\n",
      "In an endless cosmic romance,\n",
      "Black holes devour all in sight,\n",
      "While comets streak with fiery might.\n",
      "\n",
      "Astronauts brave the unknown,\n",
      "Exploring worlds yet to be shown,\n",
      "We marvel at the beauty and grace,\n",
      "Of the infinite wonders of outer space.\n",
      "\n",
      "The vastness of the cosmos so grand,\n",
      "A cosmic symphony, perfectly planned,\n",
      "So let us gaze up at the stars above,\n",
      "And dream of the mysteries we may yet solve."
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "for chunk in model.stream(\"Write a poem about space:\"):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be13ffc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c776bc4c",
   "metadata": {},
   "source": [
    "#### Imperative\n",
    "- @chain decorator is shortcut to turn any Python function into a LangChain Runnable.\n",
    "- You can now call .invoke(), .stream(), .batch() on it — just like a model or chain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3b383377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Several model providers offer Licensed Lending Models (LLMs) for financial institutions. Some of the well-known providers include Zest AI, Upstart, FactorTrust, and LenddoEFL. These providers offer LLMs that leverage alternative data and advanced analytics to help lenders make better credit decisions and expand access to credit for underserved populations. Consider reaching out to these providers to learn more about their LLM offerings and how they can benefit your institution.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# combine them in a function\n",
    "# @chain decorator adds the same Runnable interface for any function you write\n",
    "\n",
    "@chain\n",
    "def chatbot(values):\n",
    "    prompt = template.invoke(values)\n",
    "    return model.invoke(prompt)\n",
    "\n",
    "response = chatbot.invoke({\"question\": \"Which model providers offer LLMs?\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122a3146",
   "metadata": {},
   "source": [
    "@chain with stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "40469cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Many\n",
      " law\n",
      " schools\n",
      " around\n",
      " the\n",
      " world\n",
      " offer\n",
      " specialized\n",
      " LL\n",
      ".M\n",
      ".\n",
      " (\n",
      "Master\n",
      " of\n",
      " Laws\n",
      ")\n",
      " programs\n",
      ".\n",
      " Some\n",
      " well\n",
      "-known\n",
      " law\n",
      " schools\n",
      " with\n",
      " reputable\n",
      " LL\n",
      ".M\n",
      ".\n",
      " programs\n",
      " include\n",
      " Harvard\n",
      " Law\n",
      " School\n",
      ",\n",
      " Yale\n",
      " Law\n",
      " School\n",
      ",\n",
      " Stanford\n",
      " Law\n",
      " School\n",
      ",\n",
      " University\n",
      " of\n",
      " Cambridge\n",
      ",\n",
      " and\n",
      " University\n",
      " of\n",
      " Oxford\n",
      ".\n",
      " Additionally\n",
      ",\n",
      " many\n",
      " other\n",
      " reputable\n",
      " law\n",
      " schools\n",
      " and\n",
      " universities\n",
      " offer\n",
      " LL\n",
      ".M\n",
      ".\n",
      " programs\n",
      " with\n",
      " various\n",
      " special\n",
      "izations\n",
      ".\n",
      " It\n",
      "'s\n",
      " recommended\n",
      " to\n",
      " research\n",
      " specific\n",
      " law\n",
      " schools\n",
      " and\n",
      " their\n",
      " respective\n",
      " LL\n",
      ".M\n",
      ".\n",
      " programs\n",
      " to\n",
      " find\n",
      " the\n",
      " best\n",
      " fit\n",
      " for\n",
      " your\n",
      " needs\n",
      " and\n",
      " interests\n",
      ".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import chain\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "@chain\n",
    "def chatbot(values):\n",
    "    prompt = template.invoke(values)\n",
    "    for token in model.stream(prompt):\n",
    "        yield token\n",
    "\n",
    "for part in chatbot.stream({\"question\": \"Which model providers offer LLMs?\"}):\n",
    "    print(part.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b3d88d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Many law schools offer Master of Laws (LLM) programs. Some of the top law schools known for their LLM programs include Harvard Law School, Yale Law School, Stanford Law School, Columbia Law School, and NYU School of Law. Additionally, there are many other law schools worldwide that offer LLM programs in various specializations. It is advisable to research and compare different LLM programs to find the one that best suits your needs and interests.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 91, 'prompt_tokens': 25, 'total_tokens': 116, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BlH9QCEIqfXT6hnMTMS3LquAxvnxW', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--f3464597-2772-4405-be01-2d3a945853da-0', usage_metadata={'input_tokens': 25, 'output_tokens': 91, 'total_tokens': 116, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import chain\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "\n",
    "@chain\n",
    "async def chatbot(values):\n",
    "    prompt = await template.ainvoke(values)\n",
    "    return await model.ainvoke(prompt)\n",
    "\n",
    "\n",
    "await chatbot.ainvoke({\"question\": \"Which model providers offer LLMs?\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2b2fa6",
   "metadata": {},
   "source": [
    "### Simple Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "eafc59b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: The capital of France is Paris. It is known for its beautiful architecture, world-famous landmarks, delicious cuisine, and rich culture.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "prompt = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a friendly chatbot.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "filled_prompt = prompt.invoke({\"question\": \"What's the capital of France?\"})\n",
    "\n",
    "response = llm.invoke(filled_prompt)\n",
    "\n",
    "print(\"Bot:\", response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32176875",
   "metadata": {},
   "source": [
    "### ConversationChain + Memory\n",
    "- Manages memory - Remembers previous messages in a session\n",
    "- Wraps an LLM\tSends chat history + new question to LLM\n",
    "- Returns the response\tOutputs the assistant's reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "64e6f103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andi\\AppData\\Local\\Temp\\ipykernel_24544\\2249821081.py:6: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(return_messages=True)\n",
      "C:\\Users\\andi\\AppData\\Local\\Temp\\ipykernel_24544\\2249821081.py:8: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  chatbot = ConversationChain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[]\n",
      "Human: halo\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Bot: Hello! How are you doing today?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='halo', additional_kwargs={}, response_metadata={}), AIMessage(content='Hello! How are you doing today?', additional_kwargs={}, response_metadata={})]\n",
      "Human: i'm fine and you?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Bot: I'm doing well, thank you for asking! I've been learning new things and gathering information to assist you better. Is there anything specific you would like to know or talk about today?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='halo', additional_kwargs={}, response_metadata={}), AIMessage(content='Hello! How are you doing today?', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"i'm fine and you?\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm doing well, thank you for asking! I've been learning new things and gathering information to assist you better. Is there anything specific you would like to know or talk about today?\", additional_kwargs={}, response_metadata={})]\n",
      "Human: how's iran and israel friendship?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Bot: The relationship between Iran and Israel is quite complex. Historically, the two countries have not had diplomatic relations and have been involved in various conflicts and tensions. Iran does not recognize the state of Israel and has been critical of its policies towards Palestinians. On the other hand, Israel has voiced concerns about Iran's nuclear program and support for militant groups in the region. Overall, the relationship between Iran and Israel can be described as hostile and marked by mutual distrust.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "chatbot = ConversationChain(\n",
    "    llm=llm, memory=memory, verbose=True\n",
    ")\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "    response = chatbot.invoke({\"input\": user_input})\n",
    "    print(\"Bot:\", response['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de95aacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andi\\AppData\\Local\\Temp\\ipykernel_36408\\2067587560.py:20: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot_ui = gr.Chatbot()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# 1. Create LLM and memory\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "chatbot = ConversationChain(llm=llm, memory=memory)\n",
    "\n",
    "# 2. Chat handler\n",
    "def chat(user_input, history):\n",
    "    response = chatbot.invoke({\"input\": user_input})\n",
    "    history.append((user_input, response[\"response\"]))\n",
    "    return \"\", history, history  # 👈 Return \"\" to clear input\n",
    "\n",
    "# 3. Gradio UI\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# 🤖 LangChain Chatbot with Gradio\")\n",
    "    chatbot_ui = gr.Chatbot()\n",
    "    msg = gr.Textbox(label=\"Type your message\")\n",
    "    clear = gr.Button(\"Clear\")\n",
    "\n",
    "    state = gr.State([])\n",
    "\n",
    "    msg.submit(chat, [msg, state], [msg, chatbot_ui, state])  # 👈 msg updated to \"\"\n",
    "    clear.click(lambda: ([], []), None, [chatbot_ui, state])\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31de5385",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0daa8f83",
   "metadata": {},
   "source": [
    "# Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4269c6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `duckduckgo_search` with `current director of Telkom Indonesia`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mPT Telkom Indonesia (Persero) Tbk melakukan perombakan pada jajaran direksi dan ... (2007 -2011) hingga Director/Chief Digital Services Officer (2011-2014). Selain Dian, ada juga nama Angga Raka Prabowo yang menjadi Komisaris Utama menggantikan Bambang Brodjonegoro. Angga Raka saat ini menjabat sebagai Wakil Menteri Komunikasi dan Digital ... Rapat Umum Pemegang Saham Tahunan (RUPST) PT Telkom Indonesia (Persero) Tbk mengumumkan jajaran direksi dan komisaris terbaru. Berikut daftarnya. Rapat digelar pada Hari Selasa, 27 Mei 2025 yang berlokasi di Ballroom Hotel Four Seasons, Jl. Jend. Gatot Subroto No. 18 pada pukul 14.00 WIB. Hasilnya cukup mengejutkan. Telkom Indonesia (TLKM), the country's largest state-owned telecommunications company, has appointed former XL Axiata CEO Dian Siswarini as its new president director. The decision, announced during the annual general shareholders' meeting in Jakarta on Tuesday, positions Dian to replace outgoing CEO Ririek Adriansyah, who has led Telkom ... State telecommunication company PT Telkom Indonesia named Angga Raka Prabowo as its new President Commissioner during the company's Annual General Meeting of Shareholders (RUPST) in Jakarta, on Tuesday, May 27, 2025. ... as the new President Director of Telkom Indonesia. She succeeded Ririek Adriansyah, whose term had come to an end. ... JAKARTA, KOMPAS.com - PT Telkom Indonesia Tbk (TLKM) melakukan perombakan susunan dewan komisaris dan direksi dalam Rapat Umum Pemegang Saham Tahunan (RUPST), di Jakarta Selasa (27/5/2025).. RUPST mengangkat Dian Siswarini, yang merupakan mantan direktur utama XL Axiata, untuk menggantikan Ririek Ardiansyah sebagai direktur utama Telkom.. Sementara mantan dirut Angkasa Pura II Muhammad ...\u001b[0m\u001b[32;1m\u001b[1;3mThe current director of Telkom Indonesia is Dian Siswarini. She replaced the outgoing CEO Ririek Adriansyah.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'Who is the current director of Telkom Indonesia?', 'output': 'The current director of Telkom Indonesia is Dian Siswarini. She replaced the outgoing CEO Ririek Adriansyah.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_openai_functions_agent, AgentExecutor\n",
    "from langchain_core.tools import Tool\n",
    "from langchain.tools.ddg_search.tool import DuckDuckGoSearchRun\n",
    "from langchain.agents.openai_functions_agent.base import OpenAIFunctionsAgent\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "search = DuckDuckGoSearchRun()\n",
    "tool = Tool.from_function(\n",
    "    func=search,\n",
    "    name=\"duckduckgo_search\",\n",
    "    description=\"Search the web for current info.\"\n",
    ")\n",
    "\n",
    "prompt = OpenAIFunctionsAgent.create_prompt()\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4\")\n",
    "\n",
    "# Buat agent dan executor\n",
    "agent = create_openai_functions_agent(llm=llm, tools=[tool], prompt=prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=[tool], verbose=True)\n",
    "\n",
    "# Call agen\n",
    "response = agent_executor.invoke({\"input\": \"Who is the current director of Telkom Indonesia?\"})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8ace59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
