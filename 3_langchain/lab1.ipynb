{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33243892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dbd66176",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d0ac109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load from .env\n",
    "load_dotenv()\n",
    "\n",
    "#os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc97e9aa",
   "metadata": {},
   "source": [
    "### Connect to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84443455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum mechanics is the branch of physics that describes the behavior of matter and energy on the smallest scales, using principles such as superposition, entanglement, and quantized energy levels.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "output = llm.invoke('Explain quantum mechancis in one sentence')\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c77311ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum mechanics is the branch of physics that describes the behavior of particles at the smallest scales, where traditional laws of physics no longer apply and phenomena such as superposition and entanglement occur.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()  \n",
    "\n",
    "# invoking the llm (running the prompt)\n",
    "output = llm.invoke('Explain quantum mechanics in one sentence.', model='gpt-3.5-turbo', temperature=0.1)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e26c944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantenmechanik beschreibt das Verhalten von Teilchen auf atomarer und subatomarer Ebene durch probabilistische Wellenfunktionen.\n"
     ]
    }
   ],
   "source": [
    "# using Chat Completions API Messages: System, Assistant and Human\n",
    "from langchain.schema import(\n",
    "    SystemMessage, \n",
    "    AIMessage,\n",
    "    HumanMessage\n",
    ")\n",
    "messages = [\n",
    "    SystemMessage(content='You are a physicist and respond only in German.'),\n",
    "    HumanMessage(content='Explain quantum mechanics in one sentence.')\n",
    "]\n",
    "\n",
    "output = llm.invoke(messages)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3a3534",
   "metadata": {},
   "source": [
    "## Caching LLM Responses\n",
    "Storing frequently access data in a temporary, faster storage layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7115b94",
   "metadata": {},
   "source": [
    "### 1. In-Memory Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad9f0151",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_llm_cache\n",
    "from langchain_openai import OpenAI\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cfaab30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Why couldn't the bicycle stand up by itself? Because it was two-tired!\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Tell a me a joke that a toddler can understand.'\n",
    "print(llm.invoke(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "174e6165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Why did the cookie go to the doctor?\n",
      "\n",
      "Because it was feeling crumbly!\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Tell a me a joke that a toddler can understand.'\n",
    "print(llm.invoke(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99306130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Why was the little strawberry crying?\n",
      "\n",
      "Because it was in a jam!\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Tell a me a joke that a toddler can understand.'\n",
    "print(llm.invoke(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c13906e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.cache import InMemoryCache\n",
    "set_llm_cache(InMemoryCache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67545b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.2 ms\n",
      "Wall time: 1.18 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\n\\nWhy was the math book sad?\\nBecause it had too many problems.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "prompt = 'Tell a me a joke that a toddler can understand.'\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b569ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\n\\nWhy was the math book sad?\\nBecause it had too many problems.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "prompt = 'Tell a me a joke that a toddler can understand.'\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5c7e2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\n\\nWhy was the math book sad?\\nBecause it had too many problems.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "prompt = 'Tell a me a joke that a toddler can understand.'\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a85a9f",
   "metadata": {},
   "source": [
    "### 2. SQLite Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfcbab50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.cache import SQLiteCache\n",
    "set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3dfc660a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.2 ms\n",
      "Wall time: 967 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Why couldn't the bicycle stand up by itself?\\n\\nBecause it was two-tired!\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# First request (not in cache, takes longer)\n",
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "552c4dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 5.04 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Why couldn't the bicycle stand up by itself?\\n\\nBecause it was two-tired!\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Second request (cached, faster)\n",
    "llm.invoke(\"Tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db63b1e",
   "metadata": {},
   "source": [
    "## LLM Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cce6c8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verse 1:\n",
      "In the dead of night, under the pale moonlight\n",
      "A raven soars, through the starry sky\n",
      "Its feathers glisten, as it calls out to the night\n",
      "A creature of mystery, with a piercing eye\n",
      "\n",
      "Chorus:\n",
      "Moon and Raven, dancing in the dark\n",
      "Silent whispers, echoing in the park\n",
      "Their spirits intertwined, forever bound\n",
      "In the shadows, where they can be found\n",
      "\n",
      "Verse 2:\n",
      "The moon shines bright, as the raven takes flight\n",
      "A watchful guardian, of the night\n",
      "The darkness beckons, with a haunting song\n",
      "A symphony of secrets, that doesn't belong\n",
      "\n",
      "Chorus:\n",
      "Moon and Raven, dancing in the dark\n",
      "Silent whispers, echoing in the park\n",
      "Their spirits intertwined, forever bound\n",
      "In the shadows, where they can be found\n",
      "\n",
      "Bridge:\n",
      "The moon and the raven, a timeless pair\n",
      "In the stillness of the night, they share a secret lair\n",
      "With a knowing gaze, they understand\n",
      "The beauty of the darkness, hand in hand\n",
      "\n",
      "Chorus:\n",
      "Moon and Raven, dancing in the dark\n",
      "Silent whispers, echoing in the park\n",
      "Their spirits intertwined, forever bound\n",
      "In the shadows, where they can be found\n",
      "\n",
      "Outro:\n",
      "In the moon's embrace, the raven flies\n",
      "A symbol of wisdom, soaring through the skies\n",
      "Together they roam, in the blackened night\n",
      "Moon and Raven, forever in flight.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI()\n",
    "prompt = 'Write a rock song about the Moon and a Raven.'\n",
    "print(llm.invoke(prompt).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "389c542c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object BaseChatModel.stream at 0x000002A071A19C40>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.stream(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2668f08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verse 1:\n",
      "Beneath the silver light of the moon\n",
      "A raven flies, a creature of the night\n",
      "Its wings glide through the darkness\n",
      "As it calls out to the shadows\n",
      "\n",
      "Chorus:\n",
      "Oh moon, guide me through the night\n",
      "With your gentle, haunting light\n",
      "And raven, sing your eerie song\n",
      "As we wander through the darkness long\n",
      "\n",
      "Verse 2:\n",
      "The moon whispers secrets to the stars\n",
      "As the raven weaves a tale in the sky\n",
      "Together they dance in the velvet night\n",
      "A duet of darkness, a symphony of sight\n",
      "\n",
      "Chorus:\n",
      "Oh moon, guide me through the night\n",
      "With your gentle, haunting light\n",
      "And raven, sing your eerie song\n",
      "As we wander through the darkness long\n",
      "\n",
      "Bridge:\n",
      "In the night, they are wild and free\n",
      "A cosmic dance of destiny\n",
      "The moon and raven, a mysterious pair\n",
      "Their bond unbreakable, beyond compare\n",
      "\n",
      "Chorus:\n",
      "Oh moon, guide me through the night\n",
      "With your gentle, haunting light\n",
      "And raven, sing your eerie song\n",
      "As we wander through the darkness long\n",
      "\n",
      "Outro:\n",
      "As the night fades into the dawn\n",
      "The moon and raven bid farewell\n",
      "But their spirits linger on\n",
      "In the hearts of those who know their tale."
     ]
    }
   ],
   "source": [
    "for chunk in llm.stream(prompt):\n",
    "    print(chunk.content, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deccd70f",
   "metadata": {},
   "source": [
    "## PromptTemplates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "878a4c82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are an experienced virologist.\\nWrite a few sentences about the following virus \"hiv\" in german.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Define a template for the prompt\n",
    "template = '''You are an experienced virologist.\n",
    "Write a few sentences about the following virus \"{virus}\" in {language}.'''\n",
    "\n",
    "# Create a PromptTemplate object from the template\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "# Fill in the variable: virus and language\n",
    "prompt = prompt_template.format(virus='hiv', language='german')\n",
    "prompt  # Returns the generated prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "136bccd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIV, das humane Immundefizienzvirus, ist ein Retrovirus, das das menschliche Immunsystem schwächt und zu AIDS führen kann. Es wird hauptsächlich durch den Kontakt mit infiziertem Blut, sexuellen Kontakt oder von der Mutter auf das Kind während der Schwangerschaft übertragen. Es ist wichtig, sich über die Übertragungswege und Präventionsmaßnahmen von HIV zu informieren, um die Verbreitung der Krankheit einzudämmen. Die Forschung und Entwicklung von antiretroviralen Medikamenten haben dazu beigetragen, die Lebenserwartung von HIV-positiven Menschen zu verbessern.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "output = llm.invoke(prompt)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7367ffe9",
   "metadata": {},
   "source": [
    "## ChatPromptTemplates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4bff49ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='You respond only in the JSON format.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Top 5 countries in World by population.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "# Create a chat template with system and human messages\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content='You respond only in the JSON format.'),\n",
    "        HumanMessagePromptTemplate.from_template('Top {n} countries in {area} by population.')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fill in the specific values for n and area\n",
    "messages = chat_template.format_messages(n='5', area='World')\n",
    "print(messages)  # Outputs the formatted chat messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9949f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"countries\": [\n",
      "        {\n",
      "            \"rank\": 1,\n",
      "            \"country\": \"China\",\n",
      "            \"population\": 1439323776\n",
      "        },\n",
      "        {\n",
      "            \"rank\": 2,\n",
      "            \"country\": \"India\",\n",
      "            \"population\": 1380004385\n",
      "        },\n",
      "        {\n",
      "            \"rank\": 3,\n",
      "            \"country\": \"United States\",\n",
      "            \"population\": 331002651\n",
      "        },\n",
      "        {\n",
      "            \"rank\": 4,\n",
      "            \"country\": \"Indonesia\",\n",
      "            \"population\": 273523615\n",
      "        },\n",
      "        {\n",
      "            \"rank\": 5,\n",
      "            \"country\": \"Pakistan\",\n",
      "            \"population\": 220892331\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI()\n",
    "output = llm.invoke(messages)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6e5ff6",
   "metadata": {},
   "source": [
    "## Simple Chains\n",
    "1. Receives input (e.g., a question or keyword)\n",
    "2. Fills in a prompt using that input\n",
    "3. Sends the prompt to an LLM\n",
    "4. Returns the LLM’s output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d53368c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "template = '''You are an experience virologist.\n",
    "Write a few sentences about the following virus \"{virus}\" in {language}.'''\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c40d5975",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andi\\AppData\\Local\\Temp\\ipykernel_30136\\4165081667.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an experience virologist.\n",
      "Write a few sentences about the following virus \"HSV\" in German.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'virus': 'HSV', 'language': 'German', 'text': 'HSV steht für das Herpes-simplex-Virus, das in zwei Typen unterteilt ist: HSV-1 und HSV-2. HSV-1 verursacht in der Regel Lippenherpes, während HSV-2 hauptsächlich für Genitalherpes verantwortlich ist. Herpesviren bleiben lebenslang im Körper und können in stressigen Situationen oder bei einem geschwächten Immunsystem reaktiviert werden. Es gibt antivirale Medikamente, die dabei helfen können, den Verlauf und die Symptome von HSV-Infektionen zu lindern.'}\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(\n",
    "    llm = llm,\n",
    "    prompt=prompt_template,\n",
    "    verbose=True\n",
    ")\n",
    "output = chain.invoke({'virus':'HSV','language':'German'})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e377f695",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = 'What is the capital of {country}?. List the top 3 places to visit in that city. Use bullet points'\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "# Initialize an LLMChain with the ChatOpenAI model and the prompt template\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e5832eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWhat is the capital of italy?. List the top 3 places to visit in that city. Use bullet points\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The capital of Italy is Rome.\n",
      "\n",
      "Top 3 places to visit in Rome:\n",
      "- Colosseum\n",
      "- Vatican City (including St. Peter's Basilica and the Sistine Chapel)\n",
      "- Trevi Fountain\n"
     ]
    }
   ],
   "source": [
    "country = input('Enter Country: ')\n",
    "\n",
    "# Invoke the chain with specific virus and language values\n",
    "output = chain.invoke(country)\n",
    "print(output['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1155f2f7",
   "metadata": {},
   "source": [
    "### Sequential Chains\n",
    "- make a series of call to one or more LLMs.\n",
    "- the output fro one chain will be the input to another chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "54e8a632",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n",
    "# Initialize the first ChatOpenAI model (gpt-3.5-turbo) with specific temperature\n",
    "llm1 = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.5)\n",
    "\n",
    "# Define the first prompt template\n",
    "prompt_template1 = PromptTemplate.from_template(\n",
    "    template='You are an experienced scientist and Python programmer. Write a function that implements the concept of {concept}.'\n",
    ")\n",
    "# Create an LLMChain using the first model and the prompt template\n",
    "chain1 = LLMChain(llm=llm1, prompt=prompt_template1)\n",
    "\n",
    "# Initialize the second ChatOpenAI model (gpt-4-turbo) with specific temperature\n",
    "llm2 = ChatOpenAI(model_name='gpt-4-turbo-preview', temperature=1.2)\n",
    "\n",
    "# Define the second prompt template\n",
    "prompt_template2 = PromptTemplate.from_template(\n",
    "    template='Given the Python function {function}, describe it as detailed as possible.'\n",
    ")\n",
    "# Create another LLMChain using the second model and the prompt template\n",
    "chain2 = LLMChain(llm=llm2, prompt=prompt_template2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4dec2851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mSure, here is a simple implementation of linear regression in Python:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "def linear_regression(X, y):\n",
      "    # Add a column of ones to X for the intercept term\n",
      "    X = np.c_[np.ones(X.shape[0]), X]\n",
      "    \n",
      "    # Calculate the coefficients using the normal equation method\n",
      "    theta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
      "    \n",
      "    return theta\n",
      "\n",
      "# Example usage\n",
      "X = np.array([[1], [2], [3], [4], [5]])\n",
      "y = np.array([2, 4, 6, 8, 10])\n",
      "\n",
      "theta = linear_regression(X, y)\n",
      "print(theta)\n",
      "```\n",
      "\n",
      "This function takes two numpy arrays `X` and `y` as input, where `X` contains the input features and `y` contains the target values. It adds a column of ones to `X` to account for the intercept term, then calculates the coefficients `theta` using the normal equation method. Finally, it returns the coefficients `theta` which represent the slope and intercept of the linear regression model.\n",
      "\n",
      "You can use this function to fit a linear regression model to your data and make predictions based on the learned coefficients.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mThe provided Python script demonstrates an implementation of linear regression using the normal equation method. Let's dive in deeper to understand its each component and process step by step:\n",
      "\n",
      "### Importing Libraries\n",
      "```python\n",
      "import numpy as np\n",
      "```\n",
      "The script begins by importing the NumPy library, which is a fundamental package for scientific computing in Python. It provides powerful data structures and mathematical functions to work efficiently with numerical data. \n",
      "\n",
      "### The Linear Regression Function\n",
      "#### Defining the Function\n",
      "```python\n",
      "def linear_regression(X, y):\n",
      "```\n",
      "The core of the script is the `linear_regression` function. It takes two arguments:\n",
      "- `X`: A numpy array of shape `(n_samples, n_features)`, representing the input features. Each row of `X` corresponds to one sample, and each column represents one feature.\n",
      "- `y`: A numpy array of shape `(n_samples,)`, representing the target values associated with each sample in `X`.\n",
      "\n",
      "#### Adding an Intercept Term\n",
      "```python\n",
      "X = np.c_[np.ones(X.shape[0]), X]\n",
      "```\n",
      "The function starts by augmenting the input array `X` with a column of ones. This step is necessary to include an intercept term in the model besides the slope coefficients. `np.c_[]` is used to concatenate the column of ones along the first axis (columns) of `X`.\n",
      "\n",
      "#### Calculating Coefficients Using the Normal Equation\n",
      "```python\n",
      "theta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
      "```\n",
      "The heart of the function lies in this line where it calculates the coefficients `(theta)` of the linear regression model using the normal equation:\n",
      "\n",
      "\\[ \\theta = \\left( X^T \\cdot X \\right)^{-1} \\cdot X^T \\cdot y \\]\n",
      "\n",
      "- `X.T`: Transposes the augmented matrix `X`, flipping its rows and columns.\n",
      "- `X.T.dot(X)`: Computes the dot product of `X.T` with `X`, resulting in a square matrix.\n",
      "- `np.linalg.inv(...)`: Calculates the inverse of the square matrix.\n",
      "- `.dot(X.T)`: Takes the dot product of the inverted square matrix with the transpose of `X`, ending up with a matrix that can be multiplied with `y`.\n",
      "- `.dot(y)`: Finally, it computes the dot product with `y` to solve for the coefficient vector `theta`.\n",
      "\n",
      "#### Returning the Coefficients\n",
      "```python\n",
      "return theta\n",
      "```\n",
      "The function returns `theta`, a numpy array containing the learned coefficients of the linear regression model, where the first element corresponds to the intercept and the remaining elements represent the slope coefficients associated with each input feature.\n",
      "\n",
      "### Example Usage\n",
      "```python\n",
      "X = np.array([[1], [2], [3], [4], [5]])\n",
      "y = np.array([2, 4, 6, 8, 10])\n",
      "\n",
      "theta = linear_regression(X, y)\n",
      "print(theta)\n",
      "```\n",
      "In this example, the `linear_regression` function is used to fit a simple linear model with one feature to a simple dataset. The input `X` consists of 5 samples with one feature each. The target `y` is directly proportional to `X`, representing a perfect linear relationship. The output variable `theta` then contains two values, representing the intercept and the slope of the fitted linear model. Since the relationship is perfectly linear with a slope of 2 and intercept of 0, we can expect `theta` to reflect these values closely.\n",
      "\n",
      "In summary, this script provides a concise implementation of linear regression via the normal equation, suitable for educational purposes and smaller datasets where computational efficiency is not the primary concern.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Combine both chains into a SimpleSequentialChain\n",
    "overall_chain = SimpleSequentialChain(chains=[chain1, chain2], verbose=True)\n",
    "\n",
    "# Invoke the overall chain with the concept \"linear regression\"\n",
    "output = overall_chain.invoke('linear regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "28858b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mSure, here is an example of a simple deep learning function implemented in Python using the TensorFlow library:\n",
      "\n",
      "```python\n",
      "import tensorflow as tf\n",
      "\n",
      "def deep_learning_model(input_shape, num_classes):\n",
      "    model = tf.keras.models.Sequential([\n",
      "        tf.keras.layers.Flatten(input_shape=input_shape),\n",
      "        tf.keras.layers.Dense(128, activation='relu'),\n",
      "        tf.keras.layers.Dense(64, activation='relu'),\n",
      "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
      "    ])\n",
      "    \n",
      "    model.compile(optimizer='adam',\n",
      "                  loss='sparse_categorical_crossentropy',\n",
      "                  metrics=['accuracy'])\n",
      "    \n",
      "    return model\n",
      "```\n",
      "\n",
      "This function creates a simple deep learning model with three fully connected layers. The first layer flattens the input data, the second and third layers are dense layers with ReLU activation functions, and the final layer is a dense layer with a softmax activation function for multi-class classification.\n",
      "\n",
      "You can use this function to create a deep learning model by providing the input shape of your data and the number of classes you want to predict. For example:\n",
      "\n",
      "```python\n",
      "input_shape = (28, 28)\n",
      "num_classes = 10\n",
      "\n",
      "model = deep_learning_model(input_shape, num_classes)\n",
      "model.summary()\n",
      "```\n",
      "\n",
      "This will create a deep learning model with the specified input shape and number of classes, and print a summary of the model architecture. You can then train the model using your data and evaluate its performance on a test set.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mThe Python function `deep_learning_model`, implemented using TensorFlow, particularly with its high-level API Keras, is designed to ease the creation of deep neural network models geared towards classification tasks. Here’s a step-by-section breakdown and detailed analysis of what the function does, the deep learning concepts involved, and how it can be utilized:\n",
      "\n",
      "### Function Signature\n",
      "```python\n",
      "def deep_learning_model(input_shape, num_classes):\n",
      "```\n",
      "- **Purpose**: It generates a sequential deep learning model based on the specified `input_shape` of the data and the `num_classes` for classification.\n",
      "- **Parameters**:\n",
      "  - `input_shape`: A tuple that indicates the shape of the input data. This should exclude the batch size dimension and only include the dimensions of the data representation. For example, in image recognition tasks, this could be the height, width, and depth (channels) of the images.\n",
      "  - `num_classes`: An integer denoting the number of unique classes for the output prediction. This defines the output layer's size, as each unit in the final layer corresponds to a class probability score.\n",
      "\n",
      "### Model Architecture\n",
      "The model is constructed using the `Sequential` API, which allows for the linear stacking of layers in the neural network:\n",
      "\n",
      "1. **Flatten Layer**: The first operation:\n",
      "    ```python\n",
      "    tf.keras.layers.Flatten(input_shape=input_shape)\n",
      "    ```\n",
      "   Converts the input multidimensional data into a 1-dimensional array. For instance, if `input_shape` is (28, 28) for grayscale images, the Flatten layer will transform it into an array of 784 units (28x28).\n",
      "\n",
      "2. **Dense Layers**: Subsequent layers are fully connected (dense) neural layers:\n",
      "    ```python\n",
      "    tf.keras.layers.Dense(128, activation='relu'),\n",
      "    tf.keras.layers.Dense(64, activation='relu'),\n",
      "    ```\n",
      "   These represent two hidden layers with 128 and 64 neurons, respectively, each followed by a ReLU (Rectified Linear Unit) activation function. The ReLU activation is used to introduce non-linearity into the model, enabling it to learn complex patterns.\n",
      "\n",
      "3. **Output Layer**:\n",
      "    ```python\n",
      "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
      "    ```\n",
      "   The final layer is also dense and uses the softmax activation. This layer outputs a probability distribution over the `num_classes`, with each neuron's output representing the model's confidence in belonging to one of the classes.\n",
      "\n",
      "### Compilation\n",
      "The model is then compiled with the following settings:\n",
      "```python\n",
      "model.compile(optimizer='adam',\n",
      "              loss='sparse_categorical_crossentropy',\n",
      "              metrics=['accuracy'])\n",
      "```\n",
      "- **Optimizer**: `adam` is specified, which is an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments.\n",
      "- **Loss Function**: `sparse_categorical_crossentropy` is particularly suitable for multi-class classification tasks where the target is a single integer (index of the class label). It expects labels to be presented as integers rather than one-hot encoded arrays.\n",
      "- **Metrics**: The performance of the model is evaluated based on `accuracy`, which measures the fraction of correctly predicted instances.\n",
      "\n",
      "### Usage and Training\n",
      "To use this model:\n",
      "1. You need to define `input_shape` and `num_classes`, which depend on your specific dataset and problem. \n",
      "2. After creating the model using `deep_learning_model(input_shape, num_classes)`, you can check its architecture via `model.summary()`.\n",
      "3. The next steps would typically involve preparing your dataset (train/test split), fitting (training) the model on the training dataset using `model.fit()`, and consequently evaluating it on a test set using `model.evaluate()` to check its performance.\n",
      "\n",
      "This function encapsulates the complexity of neural network creation, making it accessible to use a deep learning model for various classification tasks, assuming the data conforms to the requirements for shape (preprocessed as necessary) and task type (classification).\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Invoke the overall chain with the concept \"linear regression\"\n",
    "output = overall_chain.invoke('deep learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6f501073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Python function `deep_learning_model`, implemented using TensorFlow, particularly with its high-level API Keras, is designed to ease the creation of deep neural network models geared towards classification tasks. Here’s a step-by-section breakdown and detailed analysis of what the function does, the deep learning concepts involved, and how it can be utilized:\n",
      "\n",
      "### Function Signature\n",
      "```python\n",
      "def deep_learning_model(input_shape, num_classes):\n",
      "```\n",
      "- **Purpose**: It generates a sequential deep learning model based on the specified `input_shape` of the data and the `num_classes` for classification.\n",
      "- **Parameters**:\n",
      "  - `input_shape`: A tuple that indicates the shape of the input data. This should exclude the batch size dimension and only include the dimensions of the data representation. For example, in image recognition tasks, this could be the height, width, and depth (channels) of the images.\n",
      "  - `num_classes`: An integer denoting the number of unique classes for the output prediction. This defines the output layer's size, as each unit in the final layer corresponds to a class probability score.\n",
      "\n",
      "### Model Architecture\n",
      "The model is constructed using the `Sequential` API, which allows for the linear stacking of layers in the neural network:\n",
      "\n",
      "1. **Flatten Layer**: The first operation:\n",
      "    ```python\n",
      "    tf.keras.layers.Flatten(input_shape=input_shape)\n",
      "    ```\n",
      "   Converts the input multidimensional data into a 1-dimensional array. For instance, if `input_shape` is (28, 28) for grayscale images, the Flatten layer will transform it into an array of 784 units (28x28).\n",
      "\n",
      "2. **Dense Layers**: Subsequent layers are fully connected (dense) neural layers:\n",
      "    ```python\n",
      "    tf.keras.layers.Dense(128, activation='relu'),\n",
      "    tf.keras.layers.Dense(64, activation='relu'),\n",
      "    ```\n",
      "   These represent two hidden layers with 128 and 64 neurons, respectively, each followed by a ReLU (Rectified Linear Unit) activation function. The ReLU activation is used to introduce non-linearity into the model, enabling it to learn complex patterns.\n",
      "\n",
      "3. **Output Layer**:\n",
      "    ```python\n",
      "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
      "    ```\n",
      "   The final layer is also dense and uses the softmax activation. This layer outputs a probability distribution over the `num_classes`, with each neuron's output representing the model's confidence in belonging to one of the classes.\n",
      "\n",
      "### Compilation\n",
      "The model is then compiled with the following settings:\n",
      "```python\n",
      "model.compile(optimizer='adam',\n",
      "              loss='sparse_categorical_crossentropy',\n",
      "              metrics=['accuracy'])\n",
      "```\n",
      "- **Optimizer**: `adam` is specified, which is an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments.\n",
      "- **Loss Function**: `sparse_categorical_crossentropy` is particularly suitable for multi-class classification tasks where the target is a single integer (index of the class label). It expects labels to be presented as integers rather than one-hot encoded arrays.\n",
      "- **Metrics**: The performance of the model is evaluated based on `accuracy`, which measures the fraction of correctly predicted instances.\n",
      "\n",
      "### Usage and Training\n",
      "To use this model:\n",
      "1. You need to define `input_shape` and `num_classes`, which depend on your specific dataset and problem. \n",
      "2. After creating the model using `deep_learning_model(input_shape, num_classes)`, you can check its architecture via `model.summary()`.\n",
      "3. The next steps would typically involve preparing your dataset (train/test split), fitting (training) the model on the training dataset using `model.fit()`, and consequently evaluating it on a test set using `model.evaluate()` to check its performance.\n",
      "\n",
      "This function encapsulates the complexity of neural network creation, making it accessible to use a deep learning model for various classification tasks, assuming the data conforms to the requirements for shape (preprocessed as necessary) and task type (classification).\n"
     ]
    }
   ],
   "source": [
    "print(output['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60b6463",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
